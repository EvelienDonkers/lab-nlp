{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.probability import ConditionalFreqDist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(string):\n",
    "    string = str(string)\n",
    "    string = re.sub(r'http\\S+', '', string)\n",
    "    string = string.lower()\n",
    "    pat1 = '[^\\w\\s]'\n",
    "    string = re.sub(pat1,' ', string)\n",
    "    pat2 = '[\\d]'\n",
    "    string = re.sub(pat2,' ', string)\n",
    "    string = string.replace('\\n', '')\n",
    "    return string\n",
    "\n",
    "def tokenize(s):\n",
    "    lambda s: nltk.word_tokenize(s)\n",
    "    return s\n",
    "    \n",
    "def stem_and_lemmatize(l):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ps = PorterStemmer ()\n",
    "    x = ps.stem(str((l)))\n",
    "    y = lemmatizer.lemmatize(str(x))\n",
    "    return y \n",
    "\n",
    "def remove_stopwords(l):\n",
    "    stop_words = stopwords.words('english')\n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(stop_words))\n",
    "    x = re.sub(pat,'', str(l))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_all(string):\n",
    "    string = str(string)\n",
    "    string = re.sub(r'http\\S+', '', string)\n",
    "    string = string.lower()\n",
    "    pat1 = '[^\\w\\s]'\n",
    "    string = re.sub(pat1,' ', string)\n",
    "    pat2 = '[\\d]'\n",
    "    string = re.sub(pat2,' ', string)\n",
    "    string = string.replace('\\n', '')\n",
    "    lambda string: nltk.word_tokenize(string)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ps = PorterStemmer ()\n",
    "    x = ps.stem(str((string)))\n",
    "    y = lemmatizer.lemmatize(str(x))\n",
    "    stop_words = stopwords.words('english')\n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(stop_words))\n",
    "    e = re.sub(pat,'', str(y))\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2: Sentiment Analysis\n",
    "\n",
    "In this challenge we will learn sentiment analysis and practice performing sentiment analysis on Twitter tweets. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "Sentiment analysis is to *systematically identify, extract, quantify, and study affective states and subjective information* based on texts ([reference](https://en.wikipedia.org/wiki/Sentiment_analysis)). In simple words, it's to understand whether a person is happy or unhappy in producing the piece of text. Why we (or rather, companies) care about sentiment in texts? It's because by understanding the sentiments in texts, we will be able to know if our customers are happy or unhappy about our products and services. If they are unhappy, the subsequent action is to figure out what have caused the unhappiness and make improvements.\n",
    "\n",
    "Basic sentiment analysis only understands the *positive* or *negative* (sometimes *neutral* too) polarities of the sentiment. More advanced sentiment analysis will also consider dimensions such as agreement, subjectivity, confidence, irony, and so on. In this challenge we will conduct the basic positive vs negative sentiment analysis based on real Twitter tweets.\n",
    "\n",
    "NLTK comes with a [sentiment analysis package](https://www.nltk.org/api/nltk.sentiment.html). This package is great for dummies to perform sentiment analysis because it requires only the textual data to make predictions. For example:\n",
    "\n",
    "```python\n",
    ">>> from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    ">>> txt = \"Ironhack is a Global Tech School ranked num 2 worldwide.  ",
    " ",
    "Our mission is to help people transform their careers and join a thriving community of tech professionals that love what they do.\"\n",
    ">>> analyzer = SentimentIntensityAnalyzer()\n",
    ">>> analyzer.polarity_scores(txt)\n",
    "{'neg': 0.0, 'neu': 0.741, 'pos': 0.259, 'compound': 0.8442}\n",
    "```\n",
    "\n",
    "In this challenge, however, you will not use NLTK's sentiment analysis package because in your Machine Learning training in the past 2 weeks you have learned how to make predictions more accurate than that. The [tweets data](https://www.kaggle.com/kazanova/sentiment140) we will be using today are already coded for the positive/negative sentiment. You will be able to use the Naïve Bayes classifier you learned in the lesson to predict the sentiment of tweets based on the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conducting Sentiment Analysis\n",
    "\n",
    "### Loading and Exploring Data\n",
    "\n",
    "The dataset we'll be using today is located in the lab directory named `Sentiment140.csv.zip`. You need to unzip it into a `.csv` file. Then in the cell below, load and explore the data.\n",
    "\n",
    "*Notes:* \n",
    "\n",
    "* The dataset was downloaded from [Kaggle](https://www.kaggle.com/kazanova/sentiment140). We made a slight change on the original data so that each column has a label.\n",
    "\n",
    "* The dataset is huuuuge (1.6m tweets). When you develop your data analysis codes, you can sample a subset of the data (e.g. 20k records) so that you will save a lot of time when you test your codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#everything = pd.set_index('target', 'id', 'date', 'flag', 'user', 'text') \n",
    "everything = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding = \"ISO-8859-1\",  header= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "everything.iloc[0] = ['target', 'id', 'date', 'flag', 'user', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "everything = everything.rename(columns=everything.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "everything = everything.drop([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target          id                          date      flag           user  \\\n",
       "1      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "2      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "3      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "4      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "5      0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                                text  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  \n",
       "5                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "everything.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_few_tweets = everything[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Textual Data for Sentiment Analysis\n",
    "\n",
    "Now, apply the functions you have written in Challenge 1 to your whole data set. These functions include:\n",
    "\n",
    "* `clean_up()`\n",
    "\n",
    "* `tokenize()`\n",
    "\n",
    "* `stem_and_lemmatize()`\n",
    "\n",
    "* `remove_stopwords()`\n",
    "\n",
    "Create a new column called `text_processed` in the dataframe to contain the processed data. At the end, your `text_processed` column should contain lists of word tokens that are cleaned up. Your data should look like below:\n",
    "\n",
    "![Processed Data](data-cleaning-results.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#a_few_tweets['text_processed'] = clean_up_all(a_few_tweets['text'])\n",
    "everything['text_processed']=  everything['text'].apply(lambda x: clean_up_all(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>upset     update  facebook  texting      migh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>kenichan  dived many times   ball  managed  s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>whole body feels itchy  like   fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>nationwideclass      behaving      mad       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>kwesidei   whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "      <td>need  hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "      <td>loltrish hey  long time  see  yes   rains  bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811795</td>\n",
       "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "      <td>tatiana_k nope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812025</td>\n",
       "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "      <td>twittera que  muera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812416</td>\n",
       "      <td>Mon Apr 06 22:20:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>erinx3leannexo</td>\n",
       "      <td>spring break in plain city... it's snowing</td>\n",
       "      <td>spring break  plain city      snowing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812579</td>\n",
       "      <td>Mon Apr 06 22:20:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>pardonlauren</td>\n",
       "      <td>I just re-pierced my ears</td>\n",
       "      <td>pierced  ears</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812723</td>\n",
       "      <td>Mon Apr 06 22:20:19 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TLeC</td>\n",
       "      <td>@caregiving I couldn't bear to watch it.  And ...</td>\n",
       "      <td>caregiving    bear  watch      thought  ua lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812771</td>\n",
       "      <td>Mon Apr 06 22:20:19 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>robrobbierobert</td>\n",
       "      <td>@octolinz16 It it counts, idk why I did either...</td>\n",
       "      <td>octolinz     counts  idk    either   never ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812784</td>\n",
       "      <td>Mon Apr 06 22:20:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bayofwolves</td>\n",
       "      <td>@smarrison i would've been the first, but i di...</td>\n",
       "      <td>smarrison  would    first        gun      rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812799</td>\n",
       "      <td>Mon Apr 06 22:20:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>HairByJess</td>\n",
       "      <td>@iamjazzyfizzle I wish I got to watch it with ...</td>\n",
       "      <td>iamjazzyfizzle  wish  got  watch       miss  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812964</td>\n",
       "      <td>Mon Apr 06 22:20:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lovesongwriter</td>\n",
       "      <td>Hollis' death scene will hurt me severely to w...</td>\n",
       "      <td>hollis  death scene  hurt  severely  watch  fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813137</td>\n",
       "      <td>Mon Apr 06 22:20:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>armotley</td>\n",
       "      <td>about to file taxes</td>\n",
       "      <td>file taxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813579</td>\n",
       "      <td>Mon Apr 06 22:20:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>starkissed</td>\n",
       "      <td>@LettyA ahh ive always wanted to see rent  lov...</td>\n",
       "      <td>lettya ahh ive always wanted  see rent  love ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813782</td>\n",
       "      <td>Mon Apr 06 22:20:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>gi_gi_bee</td>\n",
       "      <td>@FakerPattyPattz Oh dear. Were you drinking ou...</td>\n",
       "      <td>fakerpattypattz oh dear    drinking    forgot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813985</td>\n",
       "      <td>Mon Apr 06 22:20:37 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>quanvu</td>\n",
       "      <td>@alydesigns i was out most of the day so didn'...</td>\n",
       "      <td>alydesigns       day    get much done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813992</td>\n",
       "      <td>Mon Apr 06 22:20:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>swinspeedx</td>\n",
       "      <td>one of my friend called me, and asked to meet ...</td>\n",
       "      <td>one   friend called    asked  meet    mid vall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814119</td>\n",
       "      <td>Mon Apr 06 22:20:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cooliodoc</td>\n",
       "      <td>@angry_barista I baked you a cake but I ated it</td>\n",
       "      <td>angry_barista  baked   cake   ated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814180</td>\n",
       "      <td>Mon Apr 06 22:20:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>viJILLante</td>\n",
       "      <td>this week is not going as i had hoped</td>\n",
       "      <td>week   going    hoped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814192</td>\n",
       "      <td>Mon Apr 06 22:20:41 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Ljelli3166</td>\n",
       "      <td>blagh class at 8 tomorrow</td>\n",
       "      <td>blagh class    tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814438</td>\n",
       "      <td>Mon Apr 06 22:20:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ChicagoCubbie</td>\n",
       "      <td>I hate when I have to call and wake people up</td>\n",
       "      <td>hate     call  wake people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814783</td>\n",
       "      <td>Mon Apr 06 22:20:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>KatieAngell</td>\n",
       "      <td>Just going to cry myself to sleep after watchi...</td>\n",
       "      <td>going  cry   sleep  watching marley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1467814883</td>\n",
       "      <td>Mon Apr 06 22:20:52 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>gagoo</td>\n",
       "      <td>im sad now  Miss.Lilly</td>\n",
       "      <td>im sad   miss lilli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>1467815199</td>\n",
       "      <td>Mon Apr 06 22:20:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>abel209</td>\n",
       "      <td>ooooh.... LOL  that leslie.... and ok I won't ...</td>\n",
       "      <td>ooooh     lol   leslie      ok        leslie  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>1467815753</td>\n",
       "      <td>Mon Apr 06 22:21:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BaptisteTheFool</td>\n",
       "      <td>Meh... Almost Lover is the exception... this t...</td>\n",
       "      <td>meh    almost lover   exception     track gets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>1467815923</td>\n",
       "      <td>Mon Apr 06 22:21:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>fatkat309</td>\n",
       "      <td>some1 hacked my account on aim  now i have to ...</td>\n",
       "      <td>hacked  account  aim      make  new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599970</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578196</td>\n",
       "      <td>Tue Jun 16 08:38:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>adbillingsley</td>\n",
       "      <td>Thanks @eastwestchic &amp;amp; @wangyip Thanks! Th...</td>\n",
       "      <td>thanks  eastwestchic  amp   wangyip thanks    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599971</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578237</td>\n",
       "      <td>Tue Jun 16 08:38:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>gekkko</td>\n",
       "      <td>@marttn thanks Martin. not the most imaginativ...</td>\n",
       "      <td>marttn thanks martin     imaginative interfac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599972</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578269</td>\n",
       "      <td>Tue Jun 16 08:38:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>millerslab</td>\n",
       "      <td>@MikeJonesPhoto Congrats Mike  Way to go!</td>\n",
       "      <td>mikejonesphoto congrats mike  way  go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599973</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578319</td>\n",
       "      <td>Tue Jun 16 08:38:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>luckygeorgeblog</td>\n",
       "      <td>http://twitpic.com/7jp4n - OMG! Office Space.....</td>\n",
       "      <td>omg  office space     wanna steal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599974</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578345</td>\n",
       "      <td>Tue Jun 16 08:38:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Kristah_Diggs</td>\n",
       "      <td>@yrclndstnlvr ahaha nooo you were just away fr...</td>\n",
       "      <td>yrclndstnlvr ahaha nooo    away  everyone els...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599975</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578347</td>\n",
       "      <td>Tue Jun 16 08:38:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>CoachChic</td>\n",
       "      <td>@BizCoachDeb  Hey, I'm baack! And, thanks so m...</td>\n",
       "      <td>bizcoachdeb  hey    baack    thanks  much    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599976</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578348</td>\n",
       "      <td>Tue Jun 16 08:38:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>serianna</td>\n",
       "      <td>@mattycus Yeah, my conscience would be clear i...</td>\n",
       "      <td>mattycus yeah   conscience would  clear   case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599977</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578386</td>\n",
       "      <td>Tue Jun 16 08:38:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TeamUKskyvixen</td>\n",
       "      <td>@MayorDorisWolfe Thats my girl - dishing out t...</td>\n",
       "      <td>mayordoriswolfe thats  girl   dishing    quot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599978</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578395</td>\n",
       "      <td>Tue Jun 16 08:38:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LaurenMoo10</td>\n",
       "      <td>@shebbs123 i second that</td>\n",
       "      <td>shebbs     second</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599979</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578576</td>\n",
       "      <td>Tue Jun 16 08:38:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>angel_sammy04</td>\n",
       "      <td>In the garden</td>\n",
       "      <td>garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599980</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578679</td>\n",
       "      <td>Tue Jun 16 08:38:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>puchal_ek</td>\n",
       "      <td>@myheartandmind jo jen by nemuselo zrovna tÃ© ...</td>\n",
       "      <td>myheartandmind jo jen  nemuselo zrovna tã  ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599981</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578716</td>\n",
       "      <td>Tue Jun 16 08:38:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>youtubelatest</td>\n",
       "      <td>Another Commenting Contest! [;: Yay!!!  http:/...</td>\n",
       "      <td>another commenting contest      yay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599982</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578739</td>\n",
       "      <td>Tue Jun 16 08:38:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Mandi_Davenport</td>\n",
       "      <td>@thrillmesoon i figured out how to see my twee...</td>\n",
       "      <td>thrillmesoon  figured    see  tweets  faceboo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599983</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578758</td>\n",
       "      <td>Tue Jun 16 08:38:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xoAurixo</td>\n",
       "      <td>@oxhot theri tomorrow, drinking coffee, talkin...</td>\n",
       "      <td>oxhot theri tomorrow  drinking coffee  talkin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599984</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578847</td>\n",
       "      <td>Tue Jun 16 08:38:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RobFoxKerr</td>\n",
       "      <td>You heard it here first -- We're having a girl...</td>\n",
       "      <td>heard   first        girl  hope    looks  wen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599985</th>\n",
       "      <td>4</td>\n",
       "      <td>2193578982</td>\n",
       "      <td>Tue Jun 16 08:38:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LISKFEST</td>\n",
       "      <td>if ur the lead singer in a band, beware fallin...</td>\n",
       "      <td>ur  lead singer   band  beware falling prey  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599986</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579087</td>\n",
       "      <td>Tue Jun 16 08:38:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>marhgil</td>\n",
       "      <td>@tarayqueen too much ads on my blog.</td>\n",
       "      <td>tarayqueen  much ads   blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599987</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579092</td>\n",
       "      <td>Tue Jun 16 08:38:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cathriiin</td>\n",
       "      <td>@La_r_a NEVEER  I think that you both will get...</td>\n",
       "      <td>la_r_a neveer   think     get  well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599988</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579191</td>\n",
       "      <td>Tue Jun 16 08:38:59 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tellman</td>\n",
       "      <td>@Roy_Everitt ha- good job. that's right - we g...</td>\n",
       "      <td>roy_everitt ha  good job    right    gotta th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599989</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579211</td>\n",
       "      <td>Tue Jun 16 08:38:59 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jazzstixx</td>\n",
       "      <td>@Ms_Hip_Hop im glad ur doing well</td>\n",
       "      <td>ms_hip_hop im glad ur  well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599990</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579249</td>\n",
       "      <td>Tue Jun 16 08:38:59 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>razzberry5594</td>\n",
       "      <td>WOOOOO! Xbox is back</td>\n",
       "      <td>wooooo  xbox  back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599991</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579284</td>\n",
       "      <td>Tue Jun 16 08:38:59 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AgustinaP</td>\n",
       "      <td>@rmedina @LaTati Mmmm  That sounds absolutely ...</td>\n",
       "      <td>rmedina  latati mmmm   sounds absolutely perf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599992</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579434</td>\n",
       "      <td>Tue Jun 16 08:39:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sdancingsteph</td>\n",
       "      <td>ReCoVeRiNg FrOm ThE lOnG wEeKeNd</td>\n",
       "      <td>recovering   long weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599993</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579477</td>\n",
       "      <td>Tue Jun 16 08:39:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ChloeAmisha</td>\n",
       "      <td>@SCOOBY_GRITBOYS</td>\n",
       "      <td>scooby_gritboys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>2193579489</td>\n",
       "      <td>Tue Jun 16 08:39:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>EvolveTom</td>\n",
       "      <td>@Cliff_Forster Yeah, that does work better tha...</td>\n",
       "      <td>cliff_forster yeah    work better   waiting  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>woke     school   best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>thewdb com    cool  hear old walt interviews  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>ready   mojo makeover  ask   details</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>happy   th birthday   boo  alll time    tupac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>happy  charitytuesday  thenspcc  sparkscharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target          id                          date      flag  \\\n",
       "1            0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2            0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3            0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4            0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "5            0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "6            0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "7            0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "8            0  1467811795  Mon Apr 06 22:20:05 PDT 2009  NO_QUERY   \n",
       "9            0  1467812025  Mon Apr 06 22:20:09 PDT 2009  NO_QUERY   \n",
       "10           0  1467812416  Mon Apr 06 22:20:16 PDT 2009  NO_QUERY   \n",
       "11           0  1467812579  Mon Apr 06 22:20:17 PDT 2009  NO_QUERY   \n",
       "12           0  1467812723  Mon Apr 06 22:20:19 PDT 2009  NO_QUERY   \n",
       "13           0  1467812771  Mon Apr 06 22:20:19 PDT 2009  NO_QUERY   \n",
       "14           0  1467812784  Mon Apr 06 22:20:20 PDT 2009  NO_QUERY   \n",
       "15           0  1467812799  Mon Apr 06 22:20:20 PDT 2009  NO_QUERY   \n",
       "16           0  1467812964  Mon Apr 06 22:20:22 PDT 2009  NO_QUERY   \n",
       "17           0  1467813137  Mon Apr 06 22:20:25 PDT 2009  NO_QUERY   \n",
       "18           0  1467813579  Mon Apr 06 22:20:31 PDT 2009  NO_QUERY   \n",
       "19           0  1467813782  Mon Apr 06 22:20:34 PDT 2009  NO_QUERY   \n",
       "20           0  1467813985  Mon Apr 06 22:20:37 PDT 2009  NO_QUERY   \n",
       "21           0  1467813992  Mon Apr 06 22:20:38 PDT 2009  NO_QUERY   \n",
       "22           0  1467814119  Mon Apr 06 22:20:40 PDT 2009  NO_QUERY   \n",
       "23           0  1467814180  Mon Apr 06 22:20:40 PDT 2009  NO_QUERY   \n",
       "24           0  1467814192  Mon Apr 06 22:20:41 PDT 2009  NO_QUERY   \n",
       "25           0  1467814438  Mon Apr 06 22:20:44 PDT 2009  NO_QUERY   \n",
       "26           0  1467814783  Mon Apr 06 22:20:50 PDT 2009  NO_QUERY   \n",
       "27           0  1467814883  Mon Apr 06 22:20:52 PDT 2009  NO_QUERY   \n",
       "28           0  1467815199  Mon Apr 06 22:20:56 PDT 2009  NO_QUERY   \n",
       "29           0  1467815753  Mon Apr 06 22:21:04 PDT 2009  NO_QUERY   \n",
       "30           0  1467815923  Mon Apr 06 22:21:07 PDT 2009  NO_QUERY   \n",
       "...        ...         ...                           ...       ...   \n",
       "1599970      4  2193578196  Tue Jun 16 08:38:54 PDT 2009  NO_QUERY   \n",
       "1599971      4  2193578237  Tue Jun 16 08:38:54 PDT 2009  NO_QUERY   \n",
       "1599972      4  2193578269  Tue Jun 16 08:38:54 PDT 2009  NO_QUERY   \n",
       "1599973      4  2193578319  Tue Jun 16 08:38:55 PDT 2009  NO_QUERY   \n",
       "1599974      4  2193578345  Tue Jun 16 08:38:55 PDT 2009  NO_QUERY   \n",
       "1599975      4  2193578347  Tue Jun 16 08:38:55 PDT 2009  NO_QUERY   \n",
       "1599976      4  2193578348  Tue Jun 16 08:38:55 PDT 2009  NO_QUERY   \n",
       "1599977      4  2193578386  Tue Jun 16 08:38:55 PDT 2009  NO_QUERY   \n",
       "1599978      4  2193578395  Tue Jun 16 08:38:55 PDT 2009  NO_QUERY   \n",
       "1599979      4  2193578576  Tue Jun 16 08:38:57 PDT 2009  NO_QUERY   \n",
       "1599980      4  2193578679  Tue Jun 16 08:38:56 PDT 2009  NO_QUERY   \n",
       "1599981      4  2193578716  Tue Jun 16 08:38:57 PDT 2009  NO_QUERY   \n",
       "1599982      4  2193578739  Tue Jun 16 08:38:57 PDT 2009  NO_QUERY   \n",
       "1599983      4  2193578758  Tue Jun 16 08:38:57 PDT 2009  NO_QUERY   \n",
       "1599984      4  2193578847  Tue Jun 16 08:38:57 PDT 2009  NO_QUERY   \n",
       "1599985      4  2193578982  Tue Jun 16 08:38:58 PDT 2009  NO_QUERY   \n",
       "1599986      4  2193579087  Tue Jun 16 08:38:58 PDT 2009  NO_QUERY   \n",
       "1599987      4  2193579092  Tue Jun 16 08:38:58 PDT 2009  NO_QUERY   \n",
       "1599988      4  2193579191  Tue Jun 16 08:38:59 PDT 2009  NO_QUERY   \n",
       "1599989      4  2193579211  Tue Jun 16 08:38:59 PDT 2009  NO_QUERY   \n",
       "1599990      4  2193579249  Tue Jun 16 08:38:59 PDT 2009  NO_QUERY   \n",
       "1599991      4  2193579284  Tue Jun 16 08:38:59 PDT 2009  NO_QUERY   \n",
       "1599992      4  2193579434  Tue Jun 16 08:39:00 PDT 2009  NO_QUERY   \n",
       "1599993      4  2193579477  Tue Jun 16 08:39:00 PDT 2009  NO_QUERY   \n",
       "1599994      4  2193579489  Tue Jun 16 08:39:00 PDT 2009  NO_QUERY   \n",
       "1599995      4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996      4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997      4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998      4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999      4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \\\n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "5               joy_wolf                      @Kwesidei not the whole crew    \n",
       "6                mybirch                                        Need a hug    \n",
       "7                   coZZ  @LOLTrish hey  long time no see! Yes.. Rains a...   \n",
       "8        2Hood4Hollywood               @Tatiana_K nope they didn't have it    \n",
       "9                mimismo                          @twittera que me muera ?    \n",
       "10        erinx3leannexo        spring break in plain city... it's snowing    \n",
       "11          pardonlauren                         I just re-pierced my ears    \n",
       "12                  TLeC  @caregiving I couldn't bear to watch it.  And ...   \n",
       "13       robrobbierobert  @octolinz16 It it counts, idk why I did either...   \n",
       "14           bayofwolves  @smarrison i would've been the first, but i di...   \n",
       "15            HairByJess  @iamjazzyfizzle I wish I got to watch it with ...   \n",
       "16        lovesongwriter  Hollis' death scene will hurt me severely to w...   \n",
       "17              armotley                               about to file taxes    \n",
       "18            starkissed  @LettyA ahh ive always wanted to see rent  lov...   \n",
       "19             gi_gi_bee  @FakerPattyPattz Oh dear. Were you drinking ou...   \n",
       "20                quanvu  @alydesigns i was out most of the day so didn'...   \n",
       "21            swinspeedx  one of my friend called me, and asked to meet ...   \n",
       "22             cooliodoc   @angry_barista I baked you a cake but I ated it    \n",
       "23            viJILLante             this week is not going as i had hoped    \n",
       "24            Ljelli3166                         blagh class at 8 tomorrow    \n",
       "25         ChicagoCubbie     I hate when I have to call and wake people up    \n",
       "26           KatieAngell  Just going to cry myself to sleep after watchi...   \n",
       "27                 gagoo                             im sad now  Miss.Lilly   \n",
       "28               abel209  ooooh.... LOL  that leslie.... and ok I won't ...   \n",
       "29       BaptisteTheFool  Meh... Almost Lover is the exception... this t...   \n",
       "30             fatkat309  some1 hacked my account on aim  now i have to ...   \n",
       "...                  ...                                                ...   \n",
       "1599970    adbillingsley  Thanks @eastwestchic &amp; @wangyip Thanks! Th...   \n",
       "1599971           gekkko  @marttn thanks Martin. not the most imaginativ...   \n",
       "1599972       millerslab          @MikeJonesPhoto Congrats Mike  Way to go!   \n",
       "1599973  luckygeorgeblog  http://twitpic.com/7jp4n - OMG! Office Space.....   \n",
       "1599974    Kristah_Diggs  @yrclndstnlvr ahaha nooo you were just away fr...   \n",
       "1599975        CoachChic  @BizCoachDeb  Hey, I'm baack! And, thanks so m...   \n",
       "1599976         serianna  @mattycus Yeah, my conscience would be clear i...   \n",
       "1599977   TeamUKskyvixen  @MayorDorisWolfe Thats my girl - dishing out t...   \n",
       "1599978      LaurenMoo10                          @shebbs123 i second that    \n",
       "1599979    angel_sammy04                                     In the garden    \n",
       "1599980        puchal_ek  @myheartandmind jo jen by nemuselo zrovna tÃ© ...   \n",
       "1599981    youtubelatest  Another Commenting Contest! [;: Yay!!!  http:/...   \n",
       "1599982  Mandi_Davenport  @thrillmesoon i figured out how to see my twee...   \n",
       "1599983         xoAurixo  @oxhot theri tomorrow, drinking coffee, talkin...   \n",
       "1599984       RobFoxKerr  You heard it here first -- We're having a girl...   \n",
       "1599985         LISKFEST  if ur the lead singer in a band, beware fallin...   \n",
       "1599986          marhgil              @tarayqueen too much ads on my blog.    \n",
       "1599987        cathriiin  @La_r_a NEVEER  I think that you both will get...   \n",
       "1599988          tellman  @Roy_Everitt ha- good job. that's right - we g...   \n",
       "1599989        jazzstixx                 @Ms_Hip_Hop im glad ur doing well    \n",
       "1599990    razzberry5594                              WOOOOO! Xbox is back    \n",
       "1599991        AgustinaP  @rmedina @LaTati Mmmm  That sounds absolutely ...   \n",
       "1599992    sdancingsteph                  ReCoVeRiNg FrOm ThE lOnG wEeKeNd    \n",
       "1599993      ChloeAmisha                                  @SCOOBY_GRITBOYS    \n",
       "1599994        EvolveTom  @Cliff_Forster Yeah, that does work better tha...   \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...   \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...   \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...   \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...   \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "\n",
       "                                            text_processed  \n",
       "1         upset     update  facebook  texting      migh...  \n",
       "2         kenichan  dived many times   ball  managed  s...  \n",
       "3                     whole body feels itchy  like   fire   \n",
       "4         nationwideclass      behaving      mad       ...  \n",
       "5                                   kwesidei   whole crew   \n",
       "6                                               need  hug   \n",
       "7         loltrish hey  long time  see  yes   rains  bi...  \n",
       "8                                     tatiana_k nope        \n",
       "9                                   twittera que  muera     \n",
       "10                  spring break  plain city      snowing   \n",
       "11                                          pierced  ears   \n",
       "12        caregiving    bear  watch      thought  ua lo...  \n",
       "13        octolinz     counts  idk    either   never ta...  \n",
       "14        smarrison  would    first        gun      rea...  \n",
       "15        iamjazzyfizzle  wish  got  watch       miss  ...  \n",
       "16       hollis  death scene  hurt  severely  watch  fi...  \n",
       "17                                             file taxes   \n",
       "18        lettya ahh ive always wanted  see rent  love ...  \n",
       "19        fakerpattypattz oh dear    drinking    forgot...  \n",
       "20                  alydesigns       day    get much done   \n",
       "21       one   friend called    asked  meet    mid vall...  \n",
       "22                    angry_barista  baked   cake   ated    \n",
       "23                                  week   going    hoped   \n",
       "24                                blagh class    tomorrow   \n",
       "25                            hate     call  wake people    \n",
       "26                going  cry   sleep  watching marley       \n",
       "27                                     im sad   miss lilli  \n",
       "28       ooooh     lol   leslie      ok        leslie  ...  \n",
       "29       meh    almost lover   exception     track gets...  \n",
       "30                    hacked  account  aim      make  new   \n",
       "...                                                    ...  \n",
       "1599970  thanks  eastwestchic  amp   wangyip thanks    ...  \n",
       "1599971   marttn thanks martin     imaginative interfac...  \n",
       "1599972             mikejonesphoto congrats mike  way  go   \n",
       "1599973               omg  office space     wanna steal     \n",
       "1599974   yrclndstnlvr ahaha nooo    away  everyone els...  \n",
       "1599975   bizcoachdeb  hey    baack    thanks  much    ...  \n",
       "1599976   mattycus yeah   conscience would  clear   case    \n",
       "1599977   mayordoriswolfe thats  girl   dishing    quot...  \n",
       "1599978                                shebbs     second    \n",
       "1599979                                            garden   \n",
       "1599980   myheartandmind jo jen  nemuselo zrovna tã  ho...  \n",
       "1599981           another commenting contest      yay       \n",
       "1599982   thrillmesoon  figured    see  tweets  faceboo...  \n",
       "1599983   oxhot theri tomorrow  drinking coffee  talkin...  \n",
       "1599984   heard   first        girl  hope    looks  wen...  \n",
       "1599985   ur  lead singer   band  beware falling prey  ...  \n",
       "1599986                      tarayqueen  much ads   blog    \n",
       "1599987          la_r_a neveer   think     get  well        \n",
       "1599988   roy_everitt ha  good job    right    gotta th...  \n",
       "1599989                       ms_hip_hop im glad ur  well   \n",
       "1599990                                wooooo  xbox  back   \n",
       "1599991   rmedina  latati mmmm   sounds absolutely perf...  \n",
       "1599992                         recovering   long weekend   \n",
       "1599993                                   scooby_gritboys   \n",
       "1599994   cliff_forster yeah    work better   waiting  ...  \n",
       "1599995               woke     school   best feeling ever   \n",
       "1599996  thewdb com    cool  hear old walt interviews  ...  \n",
       "1599997              ready   mojo makeover  ask   details   \n",
       "1599998  happy   th birthday   boo  alll time    tupac ...  \n",
       "1599999  happy  charitytuesday  thenspcc  sparkscharity...  \n",
       "\n",
       "[1599999 rows x 7 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "everything['text_processed'] = everything['text_processed'].apply(lambda x: nltk.word_tokenize((x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Bag of Words\n",
    "\n",
    "The purpose of this step is to create a [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) from the processed data. The bag of words contains all the unique words in your whole text body (a.k.a. *corpus*) with the number of occurrence of each word. It will allow you to understand which words are the most important features across the whole corpus.\n",
    "\n",
    "Also, you can imagine you will have a massive set of words. The less important words (i.e. those of very low number of occurrence) do not contribute much to the sentiment. Therefore, you only need to use the most important words to build your feature set in the next step. In our case, we will use the top 5,000 words with the highest frequency to build the features.\n",
    "\n",
    "In the cell below, combine all the words in `text_processed` and calculate the frequency distribution of all words. A convenient library to calculate the term frequency distribution is NLTK's `FreqDist` class ([documentation](https://www.nltk.org/api/nltk.html#module-nltk.probability)). Then select the top 5,000 words from the frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "other = [j for i in everything['text_processed'].tolist() for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'good': 91370, 'day': 89779, 'get': 82176, 'like': 78522, 'go': 74095, 'quot': 72095, 'got': 70746, 'today': 68212, 'work': 65132, 'going': 64596, ...})"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other = nltk.FreqDist(other)\n",
    "other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Features\n",
    "\n",
    "Now let's build the features. Using the top 5,000 words, create a 2-dimensional matrix to record whether each of those words is contained in each document (tweet). Then you also have an output column to indicate whether the sentiment in each tweet is positive. For example, assuming your bag of words has 5 items (`['one', 'two', 'three', 'four', 'five']`) out of 4 documents (`['A', 'B', 'C', 'D']`), your feature set is essentially:\n",
    "\n",
    "| Doc | one | two | three | four | five | is_positive |\n",
    "|---|---|---|---|---|---|---|\n",
    "| A | True | False | False | True | False | True |\n",
    "| B | False | False | False | True | True | False |\n",
    "| C | False | True | False | False | False | True |\n",
    "| D | True | False | False | False | True | False|\n",
    "\n",
    "However, because the `nltk.NaiveBayesClassifier.train` class we will use in the next step does not work with Pandas dataframe, the structure of your feature set should be converted to the Python list looking like below:\n",
    "\n",
    "```python\n",
    "[\n",
    "\t({\n",
    "\t\t'one': True,\n",
    "\t\t'two': False,\n",
    "\t\t'three': False,\n",
    "\t\t'four': True,\n",
    "\t\t'five': False\n",
    "\t}, True),\n",
    "\t({\n",
    "\t\t'one': False,\n",
    "\t\t'two': False,\n",
    "\t\t'three': False,\n",
    "\t\t'four': True,\n",
    "\t\t'five': True\n",
    "\t}, False),\n",
    "\t({\n",
    "\t\t'one': False,\n",
    "\t\t'two': True,\n",
    "\t\t'three': False,\n",
    "\t\t'four': False,\n",
    "\t\t'five': False\n",
    "\t}, True),\n",
    "\t({\n",
    "\t\t'one': True,\n",
    "\t\t'two': False,\n",
    "\t\t'three': False,\n",
    "\t\t'four': False,\n",
    "\t\t'five': True\n",
    "\t}, False)\n",
    "]\n",
    "```\n",
    "\n",
    "To help you in this step, watch the [following video](https://www.youtube.com/watch?v=-vVskDsHcVc) to learn how to build the feature set with Python and NLTK. The source code in this video can be found [here](https://pythonprogramming.net/words-as-features-nltk-tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Building Features](building-features.jpg)](https://www.youtube.com/watch?v=-vVskDsHcVc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(other.keys())[:5000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-199-d6d2957f28eb>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-199-d6d2957f28eb>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    featuresets = (find_features(rev), category) for (rev, category) in everything['text_processed']\u001b[0m\n\u001b[1;37m                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in everything['text_processed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Traininng Naive Bayes Model\n",
    "\n",
    "In this step you will split your feature set into a training and a test set. Then you will create a Bayes classifier instance using `nltk.NaiveBayesClassifier.train` ([example](https://www.nltk.org/book/ch06.html)) to train with the training dataset.\n",
    "\n",
    "After training the model, call `classifier.show_most_informative_features()` to inspect the most important features. The output will look like:\n",
    "\n",
    "```\n",
    "Most Informative Features\n",
    "\t    snow = True            False : True   =     34.3 : 1.0\n",
    "\t  easter = True            False : True   =     26.2 : 1.0\n",
    "\t headach = True            False : True   =     20.9 : 1.0\n",
    "\t    argh = True            False : True   =     17.6 : 1.0\n",
    "\tunfortun = True            False : True   =     16.9 : 1.0\n",
    "\t    jona = True             True : False  =     16.2 : 1.0\n",
    "\t     ach = True            False : True   =     14.9 : 1.0\n",
    "\t     sad = True            False : True   =     13.0 : 1.0\n",
    "\t  parent = True            False : True   =     12.9 : 1.0\n",
    "\t  spring = True            False : True   =     12.7 : 1.0\n",
    "```\n",
    "\n",
    "The [following video](https://www.youtube.com/watch?v=rISOsUaTrO4) will help you complete this step. The source code in this video can be found [here](https://pythonprogramming.net/naive-bayes-classifier-nltk-tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Building and Training NB](nb-model-building.jpg)](https://www.youtube.com/watch?v=rISOsUaTrO4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'featuresets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-192-4b2af81e88ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# set that we'll train our classifier with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtraining_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeaturesets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#print(training_set)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# set that we'll test against.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtesting_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeaturesets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'featuresets' is not defined"
     ]
    }
   ],
   "source": [
    "# set that we'll train our classifier with\n",
    "training_set = featuresets[:2000]\n",
    "#print(training_set)\n",
    "# set that we'll test against.\n",
    "testing_set = featuresets[2000:]\n",
    "\n",
    "#classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "#print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "#classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['target', 'id', 'date', 'flag', 'user', 'text', 'text_processed']\n"
     ]
    }
   ],
   "source": [
    "print([i for i in training_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Naive Bayes Model\n",
    "\n",
    "Now we'll test our classifier with the test dataset. This is done by calling `nltk.classify.accuracy(classifier, test)`.\n",
    "\n",
    "As mentioned in one of the tutorial videos, a Naive Bayes model is considered OK if your accuracy score is over 0.6. If your accuracy score is over 0.7, you've done a great job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question 1: Improve Model Performance\n",
    "\n",
    "If you are still not exhausted so far and want to dig deeper, try to improve your classifier performance. There are many aspects you can dig into, for example:\n",
    "\n",
    "* Improve stemming and lemmatization. Inspect your bag of words and the most important features. Are there any words you should furuther remove from analysis? You can append these words to further remove to the stop words list.\n",
    "\n",
    "* Remember we only used the top 5,000 features to build model? Try using different numbers of top features. The bottom line is to use as few features as you can without compromising your model performance. The fewer features you select into your model, the faster your model is trained. Then you can use a larger sample size to improve your model accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question 2: Machine Learning Pipeline\n",
    "\n",
    "In a new Jupyter Notebook, combine all your codes into a function (or a class). Your new function will execute the complete machine learning pipeline job by receiving the dataset location and output the classifier. This will allow you to use your function to predict the sentiment of any tweet in real time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question 3: Apache Spark\n",
    "\n",
    "If you have completed the Apache Spark advanced topic lab, what you can do is to migrate your pipeline from local to a Databricks Notebook. Share your notebook with your instructor and classmates to show off your achievements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
